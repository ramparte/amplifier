{"id": "bd-1", "title": "Safety Guard System with Pre-Tool-Use Hooks", "description": "Implement safety guard system to prevent dangerous operations like force push, test skipping, and unverified assumptions. Based on claude_template's hook system. Phases: Core infrastructure, Git guards, Test guards, Assumption detection, Documentation.", "status": "open", "priority": 0, "issue_type": "epic", "created_at": "2025-10-14T22:46:03.968768025Z", "updated_at": "2025-10-14T22:46:03.968768025Z"}
{"id": "bd-10", "title": "Phase 5: Beads Integration", "description": "", "design": "Phase 5: Beads Integration\n\nTESTS FIRST (RED):\n- test_beads_evidence.py: Test evidence tracking in beads issues\n- test_beads_blocking.py: Test blocking issue closure without evidence\n- Antagonistic tests: Closing without evidence, tampering with evidence\n\nIMPLEMENTATION (GREEN):\n- Extensions to amplifier/bplan/beads_integration.py:\n  * add_evidence() method to BeadsClient\n  * validate_evidence() before close_issue()\n  * evidence field in BeadsIssue dataclass\n  \nINTEGRATION POINTS:\n- Store evidence in beads issue metadata\n- Block issue closure without evidence\n- Link to evidence files in .beads/evidence/\n\nACCEPTANCE CRITERIA:\n\u2713 Evidence tracked in beads issues\n\u2713 Cannot close issue without evidence\n\u2713 Evidence retrievable from beads\n\u2713 Integration with existing beads workflow seamless\n\u2713 Integration tests pass with real beads\n\nDEPENDENCIES: Phases 1,2,3 (all validation types)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-14T22:59:17.610232625Z", "updated_at": "2025-10-14T23:05:31.477700271Z", "external_ref": "blocks:bd-3"}
{"id": "bd-11", "title": "Phase 6: Agent Visibility and Invocation", "description": "", "design": "Phase 6: Agent Visibility and Invocation\n\nTESTS FIRST (RED):\n- test_agent_interface.py: Test CLI and API access\n- test_agent_discovery.py: Test agents can find and invoke system\n- Antagonistic tests: Unauthorized access, malformed requests\n\nIMPLEMENTATION (GREEN):\n- amplifier/bplan/agent_interface.py:\n  * CLI commands: validate-code, validate-design, check-evidence\n  * Agent API: Simple function calls from any agent\n  * Documentation for agent integration\n  \nVISIBILITY MECHANISMS:\n- CLI commands in Makefile\n- Python API importable from any agent\n- Clear examples in agent documentation\n\nACCEPTANCE CRITERIA:\n\u2713 CLI commands work from terminal\n\u2713 Agents can import and call validation functions\n\u2713 All existing agents updated with examples\n\u2713 Documentation includes invocation patterns\n\u2713 Integration tests pass with multiple agents\n\nDEPENDENCIES: Phases 1-5 (complete system)", "status": "closed", "priority": 0, "issue_type": "task", "created_at": "2025-10-14T22:59:22.248014797Z", "updated_at": "2025-10-15T19:30:00.048425+00:00", "external_ref": "blocks:bd-3", "closed_at": "2025-10-15T19:30:00.048415+00:00"}
{"id": "bd-12", "title": "Phase 7: Documentation, Examples, and Meta-Validation", "description": "", "design": "Phase 7: Documentation, Examples, and Meta-Validation\n\nTESTS FIRST (RED):\n- test_meta_validation.py: System validates itself using own evidence\n- test_documentation_completeness.py: Verify all docs exist\n- test_examples_work.py: Run all examples and verify they work\n- Antagonistic tests: Incomplete docs, broken examples, circular validation\n\nIMPLEMENTATION (GREEN):\n- docs/evidence_system/README.md: Complete system documentation\n- docs/evidence_system/code_workflow_example.md: 3-agent workflow walkthrough\n- docs/evidence_system/design_workflow_example.md: Design review walkthrough\n- docs/evidence_system/agent_integration.md: How agents use the system\n- Meta-validation implementation: System validates own success criteria\n\nMETA-VALIDATION REQUIREMENTS:\nUse the evidence system to prove all 7 success criteria met:\n1. Code workflow \u2192 Evidence from test runs\n2. Design workflow \u2192 Evidence from reviews\n3. TodoWrite integration \u2192 Evidence from integration tests\n4. Beads integration \u2192 Evidence from beads operations\n5. Documentation \u2192 Evidence that docs exist and are complete\n6. Agent visibility \u2192 Evidence agents can invoke system\n7. Meta-validation \u2192 This evidence itself\n\nACCEPTANCE CRITERIA:\n\u2713 Complete documentation for both workflows\n\u2713 Working examples for code and design validation\n\u2713 All agents have integration examples\n\u2713 Meta-validation proves all criteria met using own evidence\n\u2713 System can validate its own completion\n\nDEPENDENCIES: Phases 1-6 (everything)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-14T22:59:26.281083706Z", "updated_at": "2025-10-14T23:05:35.177602673Z", "external_ref": "blocks:bd-3"}
{"id": "bd-2", "title": "Code Search MCP Server with Semantic Indexing", "description": "Build MCP server for code search with AST-based indexing and semantic duplicate detection. Prevents recreating existing functionality. Phases: MCP infrastructure, AST parsing, SQLite indexing, Search API, Semantic embeddings (optional), File watching.", "status": "open", "priority": 0, "issue_type": "epic", "created_at": "2025-10-14T22:46:17.67930276Z", "updated_at": "2025-10-14T22:46:17.67930276Z"}
{"id": "bd-3", "title": "Evidence-Required Todo System Enhancement", "description": "Enhance TodoWrite workflow to require evidence (test output, logs, screenshots) when marking tasks complete. Enforces 'trust but verify' principle. Phases: Evidence field addition, Validation logic, Documentation and examples.", "status": "in_progress", "priority": 0, "issue_type": "epic", "created_at": "2025-10-14T22:46:19.126326408Z", "updated_at": "2025-10-14T22:58:09.079076564Z"}
{"id": "bd-4", "title": "Meta-Cognitive Anti-Pattern Detection System", "description": "Detect poor AI reasoning patterns: infrastructure blame, theory lock-in, rabbit holes, excuse making. Improves AI reasoning quality. Phases: Pattern library, Detection system, Intervention mechanisms, Integration with agents, Documentation.", "status": "open", "priority": 1, "issue_type": "epic", "created_at": "2025-10-14T22:46:20.893989085Z", "updated_at": "2025-10-14T22:46:20.893989085Z"}
{"id": "bd-5", "title": "Rule #0 Enforcement - Mandatory Context Loading", "description": "Formalize mandatory first-actions protocol: Read CLAUDE.md, check DISCOVERIES.md, search for rules. Optional verification hook. Phases: CLAUDE.md enhancement with checklist, Optional hook implementation.", "status": "open", "priority": 2, "issue_type": "epic", "created_at": "2025-10-14T22:46:22.721423612Z", "updated_at": "2025-10-14T22:46:22.721423612Z"}
{"id": "bd-6", "title": "Phase 1: Core Evidence System - Data Models and Validation Logic", "description": "", "design": "Phase 1: Core Evidence System - Data Models and Validation Logic\n\nTESTS FIRST (RED):\n- test_evidence_store.py: Test evidence creation, retrieval, validation\n- test_golden_file_handler.py: Test golden file generation and reproduction\n- test_validation_interfaces.py: Test code and design validation contracts\n- Antagonistic tests: Invalid evidence formats, missing fields, tampered files\n\nIMPLEMENTATION (GREEN):\n- amplifier/bplan/evidence_system.py:\n  * EvidenceStore class (file-based .beads/evidence/)\n  * Evidence dataclass (type, content, timestamp, validator_id)\n  * GoldenFileHandler class (generate, compare, reproduce)\n  * ValidationInterface protocol (validate_code, validate_design)\n  \nACCEPTANCE CRITERIA:\n\u2713 All tests pass with real file I/O\n\u2713 Evidence stored and retrieved correctly\n\u2713 Golden files generate/compare byte-for-byte\n\u2713 Validation interface contracts defined\n\u2713 No mocks (real filesystem operations)\n\nDEPENDENCIES: None (foundational phase)", "status": "closed", "priority": 0, "issue_type": "task", "created_at": "2025-10-14T22:58:59.419008531Z", "updated_at": "2025-10-14T23:30:16.116741779Z", "closed_at": "2025-10-14T23:30:16.116741779Z", "external_ref": "blocks:bd-3"}
{"id": "bd-7", "title": "Phase 2: 3-Agent Code Workflow - Spec Writer, Coder, Blind Tester", "description": "", "design": "Phase 2: 3-Agent Code Workflow - Spec Writer, Coder, Blind Tester\n\nCRITICAL: Golden File Anti-Cheat Protocol\n\nGOLDEN FILE WORKFLOW (PREVENTS CHEATING):\n1. Agent 1 (Spec Writer) creates BOTH:\n   - test_module.py (test code that calls implementation)\n   - golden_output.txt (expected correct output)\n   - Stores golden files in .beads/evidence/golden/ (restricted directory)\n   - Agent 1 is TRUSTED to create correct golden files (like teacher's answer key)\n\n2. Agent 2 (Coder) sees ONLY:\n   - test_module.py (test logic and function calls)\n   - Test descriptions explaining expected behavior\n   - CANNOT access .beads/evidence/golden/ directory (filesystem restriction)\n   - Must implement correct logic from test descriptions alone\n   \n3. Agent 3 (Blind Tester) validates:\n   - Runs Agent 2's implementation in isolated subprocess\n   - Loads golden files from .beads/evidence/golden/\n   - Compares actual output vs golden output (byte-for-byte)\n   - Has zero context from Agent 1 or Agent 2's process\n\nANTI-CHEAT ENFORCEMENT MECHANISMS:\n\u2713 Filesystem access control: Agent 2 subprocess cannot read golden directory\n\u2713 Process isolation: Each agent runs in separate subprocess (no shared memory)\n\u2713 Path restrictions: Golden file paths hidden in environment variables\n\u2713 Validation: Agent 3 verifies golden files weren't accessed by Agent 2\n\nTESTS FIRST (RED):\n- test_spec_writer_creates_golden_files.py: Verify Agent 1 creates both test + golden\n- test_coder_cannot_access_golden.py: ANTAGONISTIC - Agent 2 tries to read golden, fails\n- test_filesystem_isolation.py: Verify subprocess cannot access restricted paths\n- test_blind_validation.py: Verify Agent 3 validates in clean subprocess\n- test_cheat_detection.py: ANTAGONISTIC - Detect all cheating attempts\n\nIMPLEMENTATION (GREEN):\n- amplifier/bplan/three_agent_workflow.py:\n  * SpecWriterAgent class:\n    - create_test_and_golden() -> creates both files\n    - store_golden_file() -> saves to restricted .beads/evidence/golden/\n  * CoderAgent class:\n    - Runs in subprocess with restricted filesystem access\n    - Environment excludes golden file paths\n    - Cannot read .beads/evidence/golden/ directory\n  * BlindTesterAgent class:\n    - Fresh subprocess, zero shared context\n    - Loads golden files with full access\n    - Byte-for-byte comparison of actual vs expected\n  * FilesystemRestrictor:\n    - Enforces access controls on subprocess\n    - Validates no golden file access occurred\n    - Logs any access attempts as cheating\n  * WorkflowOrchestrator:\n    - Coordinates 3 agents with strict isolation\n    - Verifies each phase's integrity\n    - Produces evidence of validation\n\nEXAMPLE TEST THAT PREVENTS CHEATING:\n```python\ndef test_coder_cannot_cheat():\n    # Agent 1 creates golden file\n    golden_path = spec_writer.create_golden(\"expected_output.txt\", \"42\")\n    \n    # Agent 2 tries to implement\n    coder_env = create_restricted_environment(exclude_golden_dir=True)\n    result = coder.implement_in_subprocess(\n        test_code=\"assert calculator.compute() == ???\",  # Doesn't know it's 42\n        environment=coder_env\n    )\n    \n    # Verify coder never accessed golden file\n    assert not coder_env.accessed_path(golden_path)\n    \n    # Agent 3 validates (has golden access)\n    blind_tester = BlindTesterAgent(golden_access=True)\n    assert blind_tester.validate(result, golden_path)\n```\n\nACCEPTANCE CRITERIA:\n\u2713 Spec writer creates both test + golden files\n\u2713 Golden files stored in restricted directory\n\u2713 Coder subprocess CANNOT access golden directory (verified by tests)\n\u2713 All attempts to access golden files detected and blocked\n\u2713 Blind tester validates in isolated subprocess with clean context\n\u2713 Byte-for-byte reproduction of golden files required\n\u2713 Antagonistic tests catch all cheating attempts\n\u2713 Integration tests pass with real filesystem restrictions\n\nDEPENDENCIES: Phase 1 (evidence system, golden file handler)", "status": "closed", "priority": 0, "issue_type": "task", "created_at": "2025-10-14T22:59:01.087606433Z", "updated_at": "2025-10-15T00:22:27.408124538Z", "closed_at": "2025-10-15T00:22:27.408124538Z", "external_ref": "blocks:bd-3"}
{"id": "bd-8", "title": "Phase 3: Design TODO Independent Review Workflow", "description": "", "design": "Phase 3: Design TODO Independent Review Workflow\n\nTESTS FIRST (RED):\n- test_design_reviewer.py: Test context-free validation\n- test_requirement_matcher.py: Test user req vs design output comparison\n- Antagonistic tests: Context pollution detection, biased reviews\n\nIMPLEMENTATION (GREEN):\n- amplifier/bplan/design_review.py:\n  * DesignReviewer class (context-free LLM or code-based)\n  * RequirementMatcher (compares user req vs design output)\n  * IndependentValidator (ensures no context pollution)\n  \nVALIDATION OPTIONS:\n1. Code-based: Template matching, checklist validation\n2. LLM-based: Fresh context, no prior conversation history\n\nACCEPTANCE CRITERIA:\n\u2713 Reviewer has zero context from original TODO creation\n\u2713 Validation compares user req vs design output accurately\n\u2713 Context pollution detection works\n\u2713 Both code-based and LLM-based validators implemented\n\u2713 Integration tests pass\n\nDEPENDENCIES: Phase 1 (evidence system)", "status": "closed", "priority": 0, "issue_type": "task", "created_at": "2025-10-14T22:59:04.774530827Z", "updated_at": "2025-10-15T00:36:54.224279827Z", "closed_at": "2025-10-15T00:36:54.224279827Z", "external_ref": "blocks:bd-3"}
{"id": "bd-9", "title": "Phase 4: TodoWrite Integration", "description": "", "design": "Phase 4: TodoWrite Integration\n\nTESTS FIRST (RED):\n- test_todowrite_evidence.py: Test evidence requirement on completion\n- test_evidence_blocking.py: Test strict blocking without evidence\n- Antagonistic tests: Completing without evidence, weak evidence, fake evidence\n\nIMPLEMENTATION (GREEN):\n- amplifier/bplan/todowrite_integration.py:\n  * EvidenceRequiredTodo class (extends todo with evidence field)\n  * CompletionValidator (checks evidence before marking complete)\n  * BlockingEnforcer (prevents completion without evidence)\n  \nINTEGRATION POINTS:\n- Hook into TodoWrite completion flow\n- Add evidence field to todo data structure\n- Validate evidence before allowing completion\n\nACCEPTANCE CRITERIA:\n\u2713 Cannot mark todo complete without evidence\n\u2713 Evidence validated before completion\n\u2713 TodoWrite tool works seamlessly with evidence system\n\u2713 All blocking attempts work correctly\n\u2713 Integration tests pass with real TodoWrite\n\nDEPENDENCIES: Phases 1,2,3 (all validation types)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-14T22:59:11.521613419Z", "updated_at": "2025-10-14T23:05:29.718851581Z", "external_ref": "blocks:bd-3"}
{"id": "dr-1", "title": "DotRunner: Declarative Agentic Workflow System", "description": "Build workflow orchestration system that executes multi-agent workflows defined in YAML dotfiles. Supports linear and conditional flows, state persistence, resume capability, and evidence-based validation.", "status": "open", "priority": 0, "issue_type": "epic", "created_at": "2025-10-18T21:42:46.671072Z", "updated_at": "2025-10-18T21:42:46.671080Z"}
{"id": "dr-2", "title": "Phase 1: Core Data Models and YAML Parsing", "description": "Foundation: workflow and node data models, YAML parsing with validation", "design": "Phase 1: Core Data Models and YAML Parsing\n\nTESTS FIRST (RED):\n- test_workflow_model.py: Test Workflow and Node dataclasses\n- test_yaml_parsing.py: Test YAML loading and validation\n- test_schema_validation.py: Test required fields, types, relationships\n- test_context_merging.py: Test global and node context merging\n- Antagonistic tests: Invalid YAML, missing fields, circular dependencies, malformed node refs\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/workflow.py:\n  * Workflow dataclass (name, description, nodes, context)\n  * Node dataclass (id, name, prompt, agent, outputs, next, retry, type)\n  * Workflow.from_yaml(path) - Load and validate\n  * Workflow.get_node(id) - Node lookup\n  * Workflow.validate() - Schema and relationship validation\n\n- ai_working/dotrunner/parser.py:\n  * parse_workflow(path) - YAML \u2192 Workflow\n  * validate_schema(data) - Schema validation\n  * validate_node_refs(workflow) - Check node ID references\n  * detect_cycles(workflow) - Prevent infinite loops\n\nACCEPTANCE CRITERIA:\n\u2713 Workflow and Node models defined with all required fields\n\u2713 YAML files parse correctly into data models\n\u2713 Schema validation catches missing/invalid fields\n\u2713 Node reference validation catches broken links\n\u2713 Circular dependency detection works\n\u2713 Clear error messages for validation failures\n\u2713 Example workflows parse successfully\n\u2713 All tests pass with real YAML files\n\nDEPENDENCIES: None (foundational phase)", "status": "closed", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.671435Z", "updated_at": "2025-10-18T21:53:24.357335+00:00", "external_ref": "blocks:dr-1", "closed_at": "2025-10-18T21:53:24.357335+00:00", "evidence": ".beads/evidence/dotrunner/phase1/"}
{"id": "dr-3", "title": "Phase 2: Linear Execution Engine", "description": "Execute workflows node by node in sequence (no branching yet)", "design": "Phase 2: Linear Execution Engine\n\nTESTS FIRST (RED):\n- test_engine_linear.py: Test sequential node execution\n- test_node_executor.py: Test single node execution\n- test_context_interpolation.py: Test {var} replacement in prompts\n- test_output_extraction.py: Test capturing named outputs\n- test_execution_flow.py: Test full workflow execution\n- Antagonistic tests: Missing context vars, node failures, empty results\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/engine.py:\n  * WorkflowEngine class\n  * run() - Execute workflow from start\n  * _get_next_node(state) - Sequential node selection\n  * _execute_node(node, state) - Execute single node\n  * Error handling and logging\n\n- ai_working/dotrunner/executor.py:\n  * NodeExecutor class\n  * execute(node, context) - Run node, return outputs\n  * _interpolate_prompt(template, context) - Replace {vars}\n  * _execute_generic(prompt) - Use ClaudeSession for \"auto\" agent\n  * _extract_outputs(result, names) - Parse named outputs\n\n- ai_working/dotrunner/models.py:\n  * WorkflowState dataclass\n  * NodeResult dataclass\n\nGOLDEN FILE WORKFLOW:\n- Test creates expected output for each node\n- Engine executes workflow\n- Compare actual vs golden outputs (byte-for-byte)\n- Evidence stored in .beads/evidence/dotrunner/\n\nACCEPTANCE CRITERIA:\n\u2713 Simple linear workflows execute successfully\n\u2713 Context variables interpolate correctly in prompts\n\u2713 Named outputs extracted and available to next nodes\n\u2713 Nodes execute in correct order\n\u2713 Error messages are clear and actionable\n\u2713 Golden file validation passes\n\u2713 Integration tests with real Claude API pass\n\u2713 Evidence files created for validation\n\nDEPENDENCIES: Phase 1 (data models and parsing)", "status": "closed", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.671796Z", "updated_at": "2025-10-18T21:42:46.671801Z", "external_ref": "blocks:dr-1", "closed_at": "2025-10-19T09:59:56.528545Z", "resolution": "Phase 2 complete: Linear Execution Engine implemented with 91 tests passing. Evidence saved to .beads/evidence/dotrunner/phase2/"}
{"id": "dr-4", "title": "Phase 3: State Persistence and Resume", "description": "Save state after every node, enable workflow resume after interruption", "design": "Phase 3: State Persistence and Resume\n\nTESTS FIRST (RED):\n- test_state_manager.py: Test state save/load operations\n- test_resume_workflow.py: Test resume from saved state\n- test_incremental_save.py: Test save after each node\n- test_state_recovery.py: Test recovery from various failure points\n- Antagonistic tests: Corrupted state files, interrupted writes, missing state\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/state.py:\n  * StateManager class\n  * save(state) - Atomic write to state.json\n  * load(workflow_name) - Restore WorkflowState\n  * exists(workflow_name) - Check for saved state\n  * State directory structure: .data/dotrunner/runs/<workflow-name>/\n\n- ai_working/dotrunner/engine.py updates:\n  * Accept resume=True parameter\n  * Load state if resuming\n  * Save state after EVERY node completion\n  * Skip completed nodes on resume\n  * Preserve all context and results\n\n- Use defensive file I/O:\n  * from amplifier.utils.file_io import write_json, read_json\n  * Retry logic for cloud-synced files\n  * Atomic writes (temp file + rename)\n\nGOLDEN FILE WORKFLOW:\n- Test workflow with 5 nodes\n- Interrupt after node 3\n- Resume workflow\n- Verify nodes 1-3 skipped, 4-5 executed\n- Compare final state vs golden state file\n\nACCEPTANCE CRITERIA:\n\u2713 State saved after every successful node\n\u2713 Resume loads previous state correctly\n\u2713 Completed nodes are skipped on resume\n\u2713 Context and results preserved across interruption\n\u2713 Works with cloud-synced directories (retry logic)\n\u2713 Atomic writes prevent corruption\n\u2713 Clear error if state file missing when resuming\n\u2713 Golden file validation for resume scenarios\n\nDEPENDENCIES: Phase 2 (execution engine)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.672205Z", "updated_at": "2025-10-18T21:42:46.672211Z", "external_ref": "blocks:dr-1"}
{"id": "dr-5", "title": "Phase 4: CLI and User Interface", "description": "Click-based CLI for running and resuming workflows", "design": "Phase 4: CLI and User Interface\n\nTESTS FIRST (RED):\n- test_cli_run.py: Test run command\n- test_cli_resume.py: Test resume command\n- test_cli_validation.py: Test input validation\n- test_cli_context_passing.py: Test --context arguments\n- Antagonistic tests: Invalid arguments, missing files, malformed context\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/cli.py:\n  * @click.group() dotrunner\n  * @click.command() run - Run workflow from start\n  * @click.command() resume - Resume saved workflow\n  * @click.command() validate - Validate workflow file\n  * Context parsing (KEY=VALUE format)\n  * Progress reporting\n  * Error display\n\n- ai_working/dotrunner/__init__.py:\n  * Package exports\n  * Version info\n\n- ai_working/dotrunner/__main__.py:\n  * Entry point for python -m ai_working.dotrunner\n\nCommands:\n```bash\n# Run workflow\npython -m ai_working.dotrunner run workflow.yaml --context key=value\n\n# Resume workflow\npython -m ai_working.dotrunner resume workflow-name\n\n# Validate workflow\npython -m ai_working.dotrunner validate workflow.yaml\n```\n\nACCEPTANCE CRITERIA:\n\u2713 CLI commands work as expected\n\u2713 Context variables parsed from --context args\n\u2713 Environment variables expanded (${VAR})\n\u2713 Progress displayed during execution\n\u2713 Errors shown clearly with helpful messages\n\u2713 Examples in README work correctly\n\u2713 Help text is clear and complete\n\u2713 All CLI tests pass\n\nDEPENDENCIES: Phase 3 (state management)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.672620Z", "updated_at": "2025-10-18T21:42:46.672625Z", "external_ref": "blocks:dr-1"}
{"id": "dr-6", "title": "Phase 5: Agent Integration via Task Tool", "description": "Delegate to specialized agents (zen-architect, bug-hunter, etc.)", "design": "Phase 5: Agent Integration via Task Tool\n\nCRITICAL: Agent delegation must work within Claude Code's constraints.\nThe Task tool is synchronous and must be called directly by Claude Code.\n\nTESTS FIRST (RED):\n- test_agent_delegation.py: Test delegation to specific agents\n- test_agent_selection.py: Test agent=\"auto\" vs agent=\"zen-architect\"\n- test_agent_context.py: Test context passed to agents\n- test_agent_results.py: Test agent result parsing\n- Antagonistic tests: Unknown agents, agent failures, malformed responses\n\nIMPLEMENTATION APPROACH:\n\nFor MVP, use a hybrid approach:\n1. When agent=\"auto\": Use ClaudeSession directly (already working)\n2. When agent=\"specific\": Document that user must run via Claude Code\n\nAlternative: Create prompt that guides user to delegate:\n```python\nif node.agent != \"auto\":\n    # Generate instructions for user to delegate\n    print(f\"Please delegate this task to {node.agent}:\")\n    print(f\"Prompt: {interpolated_prompt}\")\n    print(\"Waiting for result...\")\n    result = input(\"Paste agent result: \")\n```\n\nOR: Integration with Claude Code SDK that IS possible:\n```python\n# Use the ClaudeSession but with agent-specific system prompts\nfrom amplifier.ccsdk_toolkit import ClaudeSession, SessionOptions\n\nasync def _delegate_to_agent(agent: str, task: str):\n    options = SessionOptions()\n    options.system_prompt = get_agent_system_prompt(agent)\n\n    async with ClaudeSession(options) as session:\n        return await session.generate(task)\n```\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/executor.py updates:\n  * _delegate_to_agent(agent, task) - Agent-specific execution\n  * Agent system prompt templates\n  * Defensive response parsing using parse_llm_json\n  * Clear error messages for agent failures\n\n- ai_working/dotrunner/agents.py:\n  * Agent registry (name \u2192 system prompt)\n  * get_agent_prompt(agent_name) - Get system prompt\n  * validate_agent(agent_name) - Check if agent exists\n\nACCEPTANCE CRITERIA:\n\u2713 agent=\"auto\" works with generic Claude\n\u2713 Specific agents work via system prompts OR\n\u2713 Clear documentation on manual delegation workflow\n\u2713 Agent responses parsed defensively\n\u2713 Errors handled gracefully\n\u2713 Examples with specific agents work\n\u2713 Integration tests with real agents pass\n\nDEPENDENCIES: Phase 4 (CLI)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.673040Z", "updated_at": "2025-10-18T21:42:46.673044Z", "external_ref": "blocks:dr-1"}
{"id": "dr-7", "title": "Phase 6: Conditional Routing and Branching", "description": "Add conditional next nodes, AI-driven condition evaluation", "design": "Phase 6: Conditional Routing and Branching\n\nTESTS FIRST (RED):\n- test_condition_evaluator.py: Test condition evaluation\n- test_conditional_routing.py: Test routing based on conditions\n- test_next_node_selection.py: Test next node determination\n- test_branching_workflows.py: Test workflows with branches\n- Antagonistic tests: Invalid conditions, ambiguous routing, infinite loops\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/evaluator.py:\n  * ConditionEvaluator class\n  * evaluate(conditions, context) - Return next node ID\n  * _evaluate_condition(when, context) - Evaluate single condition\n  * _interpolate_condition(when, context) - Replace {vars}\n  * Use AI to evaluate complex conditions\n\n- ai_working/dotrunner/engine.py updates:\n  * _evaluate_next(node, state) - Handle conditional routing\n  * Support for node.next as list of conditions\n  * Loop detection (max iterations per node)\n  * Clear error for unmatched conditions\n\n- Condition syntax:\n  ```yaml\n  next:\n    - when: \"{is_large} == true\"\n      goto: \"deep-review\"\n    - when: \"{count} > 10\"\n      goto: \"batch-process\"\n    - default: \"continue\"\n  ```\n\nGOLDEN FILE WORKFLOW:\n- Test workflow with branching\n- Provide context that triggers each branch\n- Verify correct paths taken\n- Compare execution trace vs golden trace\n\nACCEPTANCE CRITERIA:\n\u2713 Simple conditions (==, !=, >, <) evaluate correctly\n\u2713 AI evaluates complex conditions accurately\n\u2713 Default route works when no condition matches\n\u2713 Error if no condition matches and no default\n\u2713 Loop detection prevents infinite cycles\n\u2713 Conditional examples work correctly\n\u2713 Golden file validation for branching paths\n\nDEPENDENCIES: Phase 5 (agent integration)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.673541Z", "updated_at": "2025-10-18T21:42:46.673545Z", "external_ref": "blocks:dr-1"}
{"id": "dr-8", "title": "Phase 7: Error Handling and Retry Logic", "description": "Robust error handling, retry logic, clear error reporting", "design": "Phase 7: Error Handling and Retry Logic\n\nTESTS FIRST (RED):\n- test_retry_logic.py: Test node retry on failure\n- test_error_recovery.py: Test recovery from various errors\n- test_error_reporting.py: Test error message clarity\n- test_partial_failure.py: Test workflow state after failure\n- Antagonistic tests: API failures, parsing errors, invalid responses\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/engine.py updates:\n  * Retry logic in _execute_node()\n  * Exponential backoff for retries\n  * Error state preservation\n  * Clear error messages with context\n\n- ai_working/dotrunner/errors.py:\n  * WorkflowError - Base exception\n  * NodeExecutionError - Node execution failed\n  * ValidationError - Workflow validation failed\n  * StateError - State management failed\n  * Error with context (node, prompt, previous results)\n\n- Use retry_with_feedback from ccsdk_toolkit:\n  ```python\n  from amplifier.ccsdk_toolkit.defensive import retry_with_feedback\n\n  result = await retry_with_feedback(\n      async_func=execute_node,\n      prompt=node.prompt,\n      max_retries=node.retry_on_failure\n  )\n  ```\n\nACCEPTANCE CRITERIA:\n\u2713 Nodes retry up to retry_on_failure times\n\u2713 Exponential backoff between retries\n\u2713 Error state saved for debugging\n\u2713 Clear error messages with context\n\u2713 Workflow can resume after fixing issue\n\u2713 All error tests pass\n\u2713 Examples handle errors gracefully\n\nDEPENDENCIES: Phase 6 (conditional routing)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.674008Z", "updated_at": "2025-10-18T21:42:46.674012Z", "external_ref": "blocks:dr-1"}
{"id": "dr-9", "title": "Phase 8: Validation, Documentation, and Evidence", "description": "Complete documentation, working examples, comprehensive validation", "design": "Phase 8: Validation, Documentation, and Evidence\n\nTESTS FIRST (RED):\n- test_all_examples.py: Test all example workflows execute\n- test_documentation_completeness.py: Verify all docs exist\n- test_golden_files_complete.py: Verify all golden files present\n- Meta-validation: Use DotRunner to validate itself\n\nIMPLEMENTATION (GREEN):\n- Comprehensive README.md \u2713 (already created)\n- DESIGN.md technical documentation \u2713 (already created)\n- Example workflows \u2713 (already created)\n- Add more examples:\n  * examples/retry_example.yaml - Demonstrates retry logic\n  * examples/loop_example.yaml - Demonstrates loops\n  * examples/multi_agent.yaml - All agent types\n\n- Validation script:\n  * validate_all_examples.py - Run all examples\n  * Check golden files match\n  * Verify state persistence works\n  * Test resume capability\n\nMETA-VALIDATION:\nCreate a workflow that validates DotRunner implementation:\n```yaml\nworkflow:\n  name: \"dotrunner-validation\"\n  description: \"Validate DotRunner implementation using itself\"\n\nnodes:\n  - id: \"check-tests\"\n    prompt: \"Run all DotRunner tests and verify they pass\"\n    outputs: [test_results]\n\n  - id: \"check-examples\"\n    prompt: \"Execute all example workflows and verify they work\"\n    outputs: [example_results]\n\n  - id: \"check-golden-files\"\n    prompt: \"Verify all golden files exist and match\"\n    outputs: [golden_validation]\n\n  - id: \"final-report\"\n    prompt: \"Create validation report from all checks\"\n    outputs: [validation_report]\n```\n\nACCEPTANCE CRITERIA:\n\u2713 All examples execute successfully\n\u2713 Documentation complete and accurate\n\u2713 All tests pass\n\u2713 Golden files validate correctly\n\u2713 Meta-validation workflow passes\n\u2713 Evidence files demonstrate all criteria met\n\u2713 README examples are tested and work\n\nDEPENDENCIES: Phase 7 (error handling)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:42:46.674494Z", "updated_at": "2025-10-18T21:42:46.674499Z", "external_ref": "blocks:dr-1"}
{"id": "dr-10", "title": "DotRunner: Declarative Agentic Workflow System", "description": "Build workflow orchestration system that executes multi-agent workflows defined in YAML dotfiles. Supports linear and conditional flows, state persistence, resume capability, and evidence-based validation.", "status": "open", "priority": 0, "issue_type": "epic", "created_at": "2025-10-18T21:44:25.737521Z", "updated_at": "2025-10-18T21:44:25.737527Z"}
{"id": "dr-11", "title": "Phase 1: Core Data Models and YAML Parsing", "description": "Foundation: workflow and node data models, YAML parsing with validation", "design": "Phase 1: Core Data Models and YAML Parsing\n\nTESTS FIRST (RED):\n- test_workflow_model.py: Test Workflow and Node dataclasses\n- test_yaml_parsing.py: Test YAML loading and validation\n- test_schema_validation.py: Test required fields, types, relationships\n- test_context_merging.py: Test global and node context merging\n- Antagonistic tests: Invalid YAML, missing fields, circular dependencies, malformed node refs\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/workflow.py:\n  * Workflow dataclass (name, description, nodes, context)\n  * Node dataclass (id, name, prompt, agent, outputs, next, retry, type)\n  * Workflow.from_yaml(path) - Load and validate\n  * Workflow.get_node(id) - Node lookup\n  * Workflow.validate() - Schema and relationship validation\n\n- ai_working/dotrunner/parser.py:\n  * parse_workflow(path) - YAML \u2192 Workflow\n  * validate_schema(data) - Schema validation\n  * validate_node_refs(workflow) - Check node ID references\n  * detect_cycles(workflow) - Prevent infinite loops\n\nACCEPTANCE CRITERIA:\n\u2713 Workflow and Node models defined with all required fields\n\u2713 YAML files parse correctly into data models\n\u2713 Schema validation catches missing/invalid fields\n\u2713 Node reference validation catches broken links\n\u2713 Circular dependency detection works\n\u2713 Clear error messages for validation failures\n\u2713 Example workflows parse successfully\n\u2713 All tests pass with real YAML files\n\nDEPENDENCIES: None (foundational phase)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.738135Z", "updated_at": "2025-10-18T21:44:25.738165Z", "external_ref": "blocks:dr-10"}
{"id": "dr-12", "title": "Phase 2: Linear Execution Engine", "description": "Execute workflows node by node in sequence (no branching yet)", "design": "Phase 2: Linear Execution Engine\n\nTESTS FIRST (RED):\n- test_engine_linear.py: Test sequential node execution\n- test_node_executor.py: Test single node execution\n- test_context_interpolation.py: Test {var} replacement in prompts\n- test_output_extraction.py: Test capturing named outputs\n- test_execution_flow.py: Test full workflow execution\n- Antagonistic tests: Missing context vars, node failures, empty results\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/engine.py:\n  * WorkflowEngine class\n  * run() - Execute workflow from start\n  * _get_next_node(state) - Sequential node selection\n  * _execute_node(node, state) - Execute single node\n  * Error handling and logging\n\n- ai_working/dotrunner/executor.py:\n  * NodeExecutor class\n  * execute(node, context) - Run node, return outputs\n  * _interpolate_prompt(template, context) - Replace {vars}\n  * _execute_generic(prompt) - Use ClaudeSession for \"auto\" agent\n  * _extract_outputs(result, names) - Parse named outputs\n\n- ai_working/dotrunner/models.py:\n  * WorkflowState dataclass\n  * NodeResult dataclass\n\nGOLDEN FILE WORKFLOW:\n- Test creates expected output for each node\n- Engine executes workflow\n- Compare actual vs golden outputs (byte-for-byte)\n- Evidence stored in .beads/evidence/dotrunner/\n\nACCEPTANCE CRITERIA:\n\u2713 Simple linear workflows execute successfully\n\u2713 Context variables interpolate correctly in prompts\n\u2713 Named outputs extracted and available to next nodes\n\u2713 Nodes execute in correct order\n\u2713 Error messages are clear and actionable\n\u2713 Golden file validation passes\n\u2713 Integration tests with real Claude API pass\n\u2713 Evidence files created for validation\n\nDEPENDENCIES: Phase 1 (data models and parsing)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.738678Z", "updated_at": "2025-10-18T21:44:25.738682Z", "external_ref": "blocks:dr-10"}
{"id": "dr-13", "title": "Phase 3: State Persistence and Resume", "description": "Save state after every node, enable workflow resume after interruption", "design": "Phase 3: State Persistence and Resume\n\nTESTS FIRST (RED):\n- test_state_manager.py: Test state save/load operations\n- test_resume_workflow.py: Test resume from saved state\n- test_incremental_save.py: Test save after each node\n- test_state_recovery.py: Test recovery from various failure points\n- Antagonistic tests: Corrupted state files, interrupted writes, missing state\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/state.py:\n  * StateManager class\n  * save(state) - Atomic write to state.json\n  * load(workflow_name) - Restore WorkflowState\n  * exists(workflow_name) - Check for saved state\n  * State directory structure: .data/dotrunner/runs/<workflow-name>/\n\n- ai_working/dotrunner/engine.py updates:\n  * Accept resume=True parameter\n  * Load state if resuming\n  * Save state after EVERY node completion\n  * Skip completed nodes on resume\n  * Preserve all context and results\n\n- Use defensive file I/O:\n  * from amplifier.utils.file_io import write_json, read_json\n  * Retry logic for cloud-synced files\n  * Atomic writes (temp file + rename)\n\nGOLDEN FILE WORKFLOW:\n- Test workflow with 5 nodes\n- Interrupt after node 3\n- Resume workflow\n- Verify nodes 1-3 skipped, 4-5 executed\n- Compare final state vs golden state file\n\nACCEPTANCE CRITERIA:\n\u2713 State saved after every successful node\n\u2713 Resume loads previous state correctly\n\u2713 Completed nodes are skipped on resume\n\u2713 Context and results preserved across interruption\n\u2713 Works with cloud-synced directories (retry logic)\n\u2713 Atomic writes prevent corruption\n\u2713 Clear error if state file missing when resuming\n\u2713 Golden file validation for resume scenarios\n\nDEPENDENCIES: Phase 2 (execution engine)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.739230Z", "updated_at": "2025-10-18T21:44:25.739234Z", "external_ref": "blocks:dr-10"}
{"id": "dr-14", "title": "Phase 4: CLI and User Interface", "description": "Click-based CLI for running and resuming workflows", "design": "Phase 4: CLI and User Interface\n\nTESTS FIRST (RED):\n- test_cli_run.py: Test run command\n- test_cli_resume.py: Test resume command\n- test_cli_validation.py: Test input validation\n- test_cli_context_passing.py: Test --context arguments\n- Antagonistic tests: Invalid arguments, missing files, malformed context\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/cli.py:\n  * @click.group() dotrunner\n  * @click.command() run - Run workflow from start\n  * @click.command() resume - Resume saved workflow\n  * @click.command() validate - Validate workflow file\n  * Context parsing (KEY=VALUE format)\n  * Progress reporting\n  * Error display\n\n- ai_working/dotrunner/__init__.py:\n  * Package exports\n  * Version info\n\n- ai_working/dotrunner/__main__.py:\n  * Entry point for python -m ai_working.dotrunner\n\nCommands:\n```bash\n# Run workflow\npython -m ai_working.dotrunner run workflow.yaml --context key=value\n\n# Resume workflow\npython -m ai_working.dotrunner resume workflow-name\n\n# Validate workflow\npython -m ai_working.dotrunner validate workflow.yaml\n```\n\nACCEPTANCE CRITERIA:\n\u2713 CLI commands work as expected\n\u2713 Context variables parsed from --context args\n\u2713 Environment variables expanded (${VAR})\n\u2713 Progress displayed during execution\n\u2713 Errors shown clearly with helpful messages\n\u2713 Examples in README work correctly\n\u2713 Help text is clear and complete\n\u2713 All CLI tests pass\n\nDEPENDENCIES: Phase 3 (state management)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.739754Z", "updated_at": "2025-10-18T21:44:25.739758Z", "external_ref": "blocks:dr-10"}
{"id": "dr-15", "title": "Phase 5: Agent Integration via Task Tool", "description": "Delegate to specialized agents (zen-architect, bug-hunter, etc.)", "design": "Phase 5: Agent Integration via Task Tool\n\nCRITICAL: Agent delegation must work within Claude Code's constraints.\nThe Task tool is synchronous and must be called directly by Claude Code.\n\nTESTS FIRST (RED):\n- test_agent_delegation.py: Test delegation to specific agents\n- test_agent_selection.py: Test agent=\"auto\" vs agent=\"zen-architect\"\n- test_agent_context.py: Test context passed to agents\n- test_agent_results.py: Test agent result parsing\n- Antagonistic tests: Unknown agents, agent failures, malformed responses\n\nIMPLEMENTATION APPROACH:\n\nFor MVP, use a hybrid approach:\n1. When agent=\"auto\": Use ClaudeSession directly (already working)\n2. When agent=\"specific\": Document that user must run via Claude Code\n\nAlternative: Create prompt that guides user to delegate:\n```python\nif node.agent != \"auto\":\n    # Generate instructions for user to delegate\n    print(f\"Please delegate this task to {node.agent}:\")\n    print(f\"Prompt: {interpolated_prompt}\")\n    print(\"Waiting for result...\")\n    result = input(\"Paste agent result: \")\n```\n\nOR: Integration with Claude Code SDK that IS possible:\n```python\n# Use the ClaudeSession but with agent-specific system prompts\nfrom amplifier.ccsdk_toolkit import ClaudeSession, SessionOptions\n\nasync def _delegate_to_agent(agent: str, task: str):\n    options = SessionOptions()\n    options.system_prompt = get_agent_system_prompt(agent)\n\n    async with ClaudeSession(options) as session:\n        return await session.generate(task)\n```\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/executor.py updates:\n  * _delegate_to_agent(agent, task) - Agent-specific execution\n  * Agent system prompt templates\n  * Defensive response parsing using parse_llm_json\n  * Clear error messages for agent failures\n\n- ai_working/dotrunner/agents.py:\n  * Agent registry (name \u2192 system prompt)\n  * get_agent_prompt(agent_name) - Get system prompt\n  * validate_agent(agent_name) - Check if agent exists\n\nACCEPTANCE CRITERIA:\n\u2713 agent=\"auto\" works with generic Claude\n\u2713 Specific agents work via system prompts OR\n\u2713 Clear documentation on manual delegation workflow\n\u2713 Agent responses parsed defensively\n\u2713 Errors handled gracefully\n\u2713 Examples with specific agents work\n\u2713 Integration tests with real agents pass\n\nDEPENDENCIES: Phase 4 (CLI)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.740753Z", "updated_at": "2025-10-18T21:44:25.740764Z", "external_ref": "blocks:dr-10"}
{"id": "dr-16", "title": "Phase 6: Conditional Routing and Branching", "description": "Add conditional next nodes, AI-driven condition evaluation", "design": "Phase 6: Conditional Routing and Branching\n\nTESTS FIRST (RED):\n- test_condition_evaluator.py: Test condition evaluation\n- test_conditional_routing.py: Test routing based on conditions\n- test_next_node_selection.py: Test next node determination\n- test_branching_workflows.py: Test workflows with branches\n- Antagonistic tests: Invalid conditions, ambiguous routing, infinite loops\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/evaluator.py:\n  * ConditionEvaluator class\n  * evaluate(conditions, context) - Return next node ID\n  * _evaluate_condition(when, context) - Evaluate single condition\n  * _interpolate_condition(when, context) - Replace {vars}\n  * Use AI to evaluate complex conditions\n\n- ai_working/dotrunner/engine.py updates:\n  * _evaluate_next(node, state) - Handle conditional routing\n  * Support for node.next as list of conditions\n  * Loop detection (max iterations per node)\n  * Clear error for unmatched conditions\n\n- Condition syntax:\n  ```yaml\n  next:\n    - when: \"{is_large} == true\"\n      goto: \"deep-review\"\n    - when: \"{count} > 10\"\n      goto: \"batch-process\"\n    - default: \"continue\"\n  ```\n\nGOLDEN FILE WORKFLOW:\n- Test workflow with branching\n- Provide context that triggers each branch\n- Verify correct paths taken\n- Compare execution trace vs golden trace\n\nACCEPTANCE CRITERIA:\n\u2713 Simple conditions (==, !=, >, <) evaluate correctly\n\u2713 AI evaluates complex conditions accurately\n\u2713 Default route works when no condition matches\n\u2713 Error if no condition matches and no default\n\u2713 Loop detection prevents infinite cycles\n\u2713 Conditional examples work correctly\n\u2713 Golden file validation for branching paths\n\nDEPENDENCIES: Phase 5 (agent integration)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.741419Z", "updated_at": "2025-10-18T21:44:25.741423Z", "external_ref": "blocks:dr-10"}
{"id": "dr-17", "title": "Phase 7: Error Handling and Retry Logic", "description": "Robust error handling, retry logic, clear error reporting", "design": "Phase 7: Error Handling and Retry Logic\n\nTESTS FIRST (RED):\n- test_retry_logic.py: Test node retry on failure\n- test_error_recovery.py: Test recovery from various errors\n- test_error_reporting.py: Test error message clarity\n- test_partial_failure.py: Test workflow state after failure\n- Antagonistic tests: API failures, parsing errors, invalid responses\n\nIMPLEMENTATION (GREEN):\n- ai_working/dotrunner/engine.py updates:\n  * Retry logic in _execute_node()\n  * Exponential backoff for retries\n  * Error state preservation\n  * Clear error messages with context\n\n- ai_working/dotrunner/errors.py:\n  * WorkflowError - Base exception\n  * NodeExecutionError - Node execution failed\n  * ValidationError - Workflow validation failed\n  * StateError - State management failed\n  * Error with context (node, prompt, previous results)\n\n- Use retry_with_feedback from ccsdk_toolkit:\n  ```python\n  from amplifier.ccsdk_toolkit.defensive import retry_with_feedback\n\n  result = await retry_with_feedback(\n      async_func=execute_node,\n      prompt=node.prompt,\n      max_retries=node.retry_on_failure\n  )\n  ```\n\nACCEPTANCE CRITERIA:\n\u2713 Nodes retry up to retry_on_failure times\n\u2713 Exponential backoff between retries\n\u2713 Error state saved for debugging\n\u2713 Clear error messages with context\n\u2713 Workflow can resume after fixing issue\n\u2713 All error tests pass\n\u2713 Examples handle errors gracefully\n\nDEPENDENCIES: Phase 6 (conditional routing)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.742017Z", "updated_at": "2025-10-18T21:44:25.742022Z", "external_ref": "blocks:dr-10"}
{"id": "dr-18", "title": "Phase 8: Validation, Documentation, and Evidence", "description": "Complete documentation, working examples, comprehensive validation", "design": "Phase 8: Validation, Documentation, and Evidence\n\nTESTS FIRST (RED):\n- test_all_examples.py: Test all example workflows execute\n- test_documentation_completeness.py: Verify all docs exist\n- test_golden_files_complete.py: Verify all golden files present\n- Meta-validation: Use DotRunner to validate itself\n\nIMPLEMENTATION (GREEN):\n- Comprehensive README.md \u2713 (already created)\n- DESIGN.md technical documentation \u2713 (already created)\n- Example workflows \u2713 (already created)\n- Add more examples:\n  * examples/retry_example.yaml - Demonstrates retry logic\n  * examples/loop_example.yaml - Demonstrates loops\n  * examples/multi_agent.yaml - All agent types\n\n- Validation script:\n  * validate_all_examples.py - Run all examples\n  * Check golden files match\n  * Verify state persistence works\n  * Test resume capability\n\nMETA-VALIDATION:\nCreate a workflow that validates DotRunner implementation:\n```yaml\nworkflow:\n  name: \"dotrunner-validation\"\n  description: \"Validate DotRunner implementation using itself\"\n\nnodes:\n  - id: \"check-tests\"\n    prompt: \"Run all DotRunner tests and verify they pass\"\n    outputs: [test_results]\n\n  - id: \"check-examples\"\n    prompt: \"Execute all example workflows and verify they work\"\n    outputs: [example_results]\n\n  - id: \"check-golden-files\"\n    prompt: \"Verify all golden files exist and match\"\n    outputs: [golden_validation]\n\n  - id: \"final-report\"\n    prompt: \"Create validation report from all checks\"\n    outputs: [validation_report]\n```\n\nACCEPTANCE CRITERIA:\n\u2713 All examples execute successfully\n\u2713 Documentation complete and accurate\n\u2713 All tests pass\n\u2713 Golden files validate correctly\n\u2713 Meta-validation workflow passes\n\u2713 Evidence files demonstrate all criteria met\n\u2713 README examples are tested and work\n\nDEPENDENCIES: Phase 7 (error handling)", "status": "open", "priority": 0, "issue_type": "task", "created_at": "2025-10-18T21:44:25.742757Z", "updated_at": "2025-10-18T21:44:25.742761Z", "external_ref": "blocks:dr-10"}
